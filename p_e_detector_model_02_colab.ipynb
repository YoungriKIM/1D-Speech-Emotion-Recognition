{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "b18ecf447b39d25eae6a9ae9d5694327d58d99c869301a8afbbe0bb2ae2074bb"
    },
    "colab": {
      "name": "p_e_detector_model_02_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungriKIM/1D-Speech-Emotion-Recognition/blob/master/p_e_detector_model_02_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPHeA03wkXql"
      },
      "source": [
        "# 화자 추가 학습에 따른 가중치 비교용 모델 작성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wD5rymGpF2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d312879e-6d0d-498a-c453-d18696f2061b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cons8nKskXqm",
        "outputId": "7996a9f4-b1cf-4237-bcb6-ac3dab0e0f14"
      },
      "source": [
        "# 라이브러리\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, LSTM, Dense, Activation, Layer\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, Flatten, LeakyReLU\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "! pip install keras_self_attention\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "import datetime\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_self_attention in /usr/local/lib/python3.7/dist-packages (0.50.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras_self_attention) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXuVliMVkXqn"
      },
      "source": [
        "# 랜덤시드 고정 -------------------------------\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "# --------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ufyQUL8kXqn",
        "outputId": "66a61eb4-fb2f-40e0-d35b-eaafeb488536"
      },
      "source": [
        "# 데이터 불러와서 합치기\n",
        "\n",
        "p_x_tr_df = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch_x_tr_df.npy')\n",
        "e_x_tr_df = np.load('/content/drive/MyDrive/SER/kesdy18/npy/energy_x_tr_df.npy')\n",
        "p_x_t_df = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch_x_t_df.npy')\n",
        "e_x_t_df = np.load('/content/drive/MyDrive/SER/kesdy18/npy/energy_x_t_df.npy')\n",
        "\n",
        "print(p_x_t_df.shape)\n",
        "print(e_x_t_df.shape)\n",
        "\n",
        "x_tr = np.concatenate((p_x_tr_df, e_x_tr_df), axis=1)\n",
        "y_tr = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch/15_kesd_ind_pitch_y_train.npy', allow_pickle=True)\n",
        "y_tr_n = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch/15_kesd_ind_pitch_train_name.npy', allow_pickle=True)\n",
        "\n",
        "x_t = np.concatenate((p_x_t_df, e_x_t_df), axis=1)\n",
        "y_t = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch/15_kesd_ind_pitch_y_test.npy', allow_pickle=True)\n",
        "y_t_n = np.load('/content/drive/MyDrive/SER/kesdy18/npy/pitch/15_kesd_ind_pitch_test_name.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "x_tr = x_tr.reshape(x_tr.shape[0], x_tr.shape[1], 1)\n",
        "x_t = x_t.reshape(x_t.shape[0], x_t.shape[1], 1)\n",
        "\n",
        "print(x_tr.shape, y_tr.shape, y_tr_n.shape)\n",
        "print(x_t.shape, y_t.shape, y_t_n.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(480, 4)\n",
            "(480, 5)\n",
            "(1920, 9, 1) (1920, 4) (1920,)\n",
            "(480, 9, 1) (480, 4) (480,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26ABLaEYT0WO"
      },
      "source": [
        "# https://github.com/YoungriKIM/STUDY/blob/d506036d0d0a19334fd2fe5ab0e6ec52f0c10839/ml/m17_nested_cv.py\n",
        "# 랜덤시드 고정 -------------------------------\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "# --------------------------------------------\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.testing import all_estimators \n",
        "from sklearn.datasets import load_iris \n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')  \n",
        "\n",
        "X = np.concatenate((x_tr, x_t), axis=0)\n",
        "Y = np.concatenate((y_tr, y_t), axis=0)\n",
        "X = X.reshape(X.shape[0], X.shape[1])\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "parameters = [  \n",
        "    {'n_estimators' : [100,200], 'n_jobs' : [-1]},\n",
        "    {'max_depth' : [6,8,10,12], 'min_samples_split' : [2,3,5,10]},\n",
        "    {'min_samples_leaf' : [3,5,7,10]},\n",
        "    {'min_samples_split' : [2,3,5,10], 'max_depth' : [6,8,10,12]},\n",
        "    {'n_jobs' : [-1], 'min_samples_leaf' : [3,5,7,10]}\n",
        "]\n",
        "\n",
        "#2. 모델 구성\n",
        "model = GridSearchCV(RandomForestClassifier(), parameters, cv = kfold)\n",
        "\n",
        "score = cross_val_score(model, X, Y, cv=kfold) \n",
        "\n",
        "print('교차검증점수: ', score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiOhBhiCkXqo",
        "outputId": "02036b5c-01b4-4e17-df0f-e36cf13fbb37"
      },
      "source": [
        "# 데이터 화자 확인용\n",
        "def check_act(data):\n",
        "    check_list=[]\n",
        "    for i in data:\n",
        "        if i[:3] not in check_list:\n",
        "            check_list.append(i[:3])\n",
        "    print(check_list)\n",
        "\n",
        "print('x_tr 의 화자: ', end='');check_act(y_tr_n)\n",
        "print('x_t 의 화자: ', end='');check_act(y_t_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_tr 의 화자: ['004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030']\n",
            "x_t 의 화자: ['001', '002', '003', '016', '017', '018']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnpyfGCKkXqq"
      },
      "source": [
        "h5_path = '/content/drive/MyDrive/h5/p_e_detector_model_01.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4E-Fm5jkXqq"
      },
      "source": [
        "def train(model, x_tr, y_tr, epochs, batch_size, h5_path):\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=16)\n",
        "    lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, mode='min')\n",
        "    mc = ModelCheckpoint(filepath=h5_path, monitor='val_loss', mode='min', verbose=1,\n",
        "                         save_best_only=True)\n",
        "    tb = TensorBoard(log_dir='D:/yespeechdata/graph/' + h5_path.split('/')[-1][:-3] + \"/\",histogram_freq=0, write_graph=True, write_images=True)\n",
        "    \n",
        "    history = model.fit(x_tr, y_tr, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[es, mc, tb, lr])\n",
        "\n",
        "    print('loss : ', history.history['loss'][-1])\n",
        "    print('acc : ', history.history['accuracy'][-1])\n",
        "    print('val loss : ', history.history['val_loss'][-1])\n",
        "    print('val acc : ', history.history['val_accuracy'][-1])\n",
        "    return model\n",
        "\n",
        "def test(model, x_t, y_t):\n",
        "    saved_model = load_model(model, custom_objects={'SeqSelfAttention':SeqSelfAttention})\n",
        "    print(x_t.shape)\n",
        "    print(y_t.shape)\n",
        "    score = saved_model.evaluate(x_t, y_t, batch_size=16)\n",
        "    print('test loss : ', score[0])\n",
        "    print('test acc : ', score[1])\n",
        "\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T7wsW-bkXqr"
      },
      "source": [
        "def cf_make(cf_matrix):\n",
        "    \n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # 시각화\n",
        "    categories = ['anger','happiness', 'neutral', 'sadness']\n",
        "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
        "    group_percentages = []\n",
        "    for i in cf_matrix:\n",
        "        group_percentage = [\"{0:.2%}\".format(value) for value in i.flatten()/np.sum(i)]\n",
        "        group_percentages.append(group_percentage)\n",
        "    \n",
        "    group_percentageslist = sum(group_percentages, [])\n",
        "\n",
        "    # average acc 구하기 -------------------\n",
        "    sumcm = 0\n",
        "    cate = cf_matrix.shape[0]\n",
        "    for j in range(cate):\n",
        "        for k in range(cate):\n",
        "            if j == k:\n",
        "                thenum = group_percentages[j][k]\n",
        "                sumcm += float(thenum[:-1])\n",
        "                aveacc = sumcm/cate\n",
        "    print('average acc: ', round(aveacc, 2),'%')\n",
        "    # -------------------------------------\n",
        "    # 시각화 이어서 진행\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts,group_percentageslist)]\n",
        "    labels = np.asarray(labels).reshape(4,4)\n",
        "    plt.subplots(figsize=(10,6))\n",
        "    plt.title('confusion_matrix')\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "HcseCHnvkXqs",
        "outputId": "b3316372-a54c-4df3-ee46-5ab7698c93ae"
      },
      "source": [
        "'''\n",
        "# 랜덤시드 고정 -------------------------------\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "# --------------------------------------------\n",
        "\n",
        "# 시간 측정\n",
        "str_time = datetime.datetime.now()\n",
        "\n",
        "num_batch_size = 16\n",
        "num_epochs = 1500\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# define model\n",
        "model = emo1d(input_shape=x_tr.shape[1:], num_classes=len(np.unique(np.argmax(y_tr, 1))), lr = learning_rate)\n",
        "# model.summary()\n",
        "\n",
        "# train model\n",
        "model = train(model, x_tr, y_tr, num_epochs, num_batch_size, h5_path)\n",
        "\n",
        "# test model\n",
        "print('='*100)\n",
        "score = test(h5_path, x_t, y_t)\n",
        "\n",
        "# predict model\n",
        "saved_model = load_model(h5_path, custom_objects={'SeqSelfAttention':SeqSelfAttention})\n",
        "y_p = saved_model.predict(x_t)\n",
        "y_p = np.argmax(y_p, axis=1)\n",
        "y_t = np.argmax(y_t, axis=1)\n",
        "cf_matrix = confusion_matrix(y_t, y_p)\n",
        "cf_make(cf_matrix)\n",
        "\n",
        "# 시간 측정\n",
        "print('모델 학습이 완료되었습니다.')\n",
        "print('time: ', datetime.datetime.now() - str_time)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# 랜덤시드 고정 -------------------------------\\nSEED = 42\\nos.environ['PYTHONHASHSEED'] = str(SEED)\\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\\ntf.random.set_seed(SEED)\\nnp.random.seed(SEED)\\nrandom.seed(SEED)\\n# --------------------------------------------\\n\\n# 시간 측정\\nstr_time = datetime.datetime.now()\\n\\nnum_batch_size = 16\\nnum_epochs = 1500\\nlearning_rate = 0.0001\\n\\n# define model\\nmodel = emo1d(input_shape=x_tr.shape[1:], num_classes=len(np.unique(np.argmax(y_tr, 1))), lr = learning_rate)\\n# model.summary()\\n\\n# train model\\nmodel = train(model, x_tr, y_tr, num_epochs, num_batch_size, h5_path)\\n\\n# test model\\nprint('='*100)\\nscore = test(h5_path, x_t, y_t)\\n\\n# predict model\\nsaved_model = load_model(h5_path, custom_objects={'SeqSelfAttention':SeqSelfAttention})\\ny_p = saved_model.predict(x_t)\\ny_p = np.argmax(y_p, axis=1)\\ny_t = np.argmax(y_t, axis=1)\\ncf_matrix = confusion_matrix(y_t, y_p)\\ncf_make(cf_matrix)\\n\\n# 시간 측정\\nprint('모델 학습이 완료되었습니다.')\\nprint('time: ', datetime.datetime.now() - str_time)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HWb6O4UTyRC"
      },
      "source": [
        "# ================================================================================="
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}